{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento y evaluación modelo de reconocimiento de caras con Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea de esta notebook es realizar el entrenamiento y evaluación de un modelo mediane tensorflow y keras.\n",
    "\n",
    "Las imagenes para ajustar y evaluar los modelos son las mismas de la notebook \"01_entrenar_modelo\\01_entrenar_modelo.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import re    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define los parámetros de ejecución de la notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros notebook\n",
    "DIM = 30\n",
    "seed = 42\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define los modelos a entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# planteamos distintas pruebas\n",
    "\n",
    "#n_pca: cantidad de componentes principales a considerar\n",
    "#excl_n_prim_comp: excluir las primeras excl_n_prim_comp componenetes principales (0 no se excluye ninguna)\n",
    "#nueronas_layer_1: neuronas capa oculta 1\n",
    "#nueronas_layer_2: neuronas capa oculta 2\n",
    "#n_epochs: cantidad de epochs... NO SE TOMA EN CUENTA... SE HARDCODEA MÁS ABAJO!!!\n",
    "\n",
    "# PARA LA ULTIMA PARTE DE LA NOTEBOOK ES IMPORTANTE TENER A MANO LOS PARAMETROS\n",
    "# \"n_pca\" y \"excl_n_prim_comp\" del modelo seleccionado!!!\n",
    "\n",
    "pruebas = {\n",
    "    \"prueba_1\": {\"n_pca\":60, \"excl_n_prim_comp\":5, \"nueronas_layer_1\":40, \"nueronas_layer_2\":25, \"n_epochs\":20},\n",
    "    #\"prueba_2\": {\"n_pca\":100, \"excl_n_prim_comp\":0, \"nueronas_layer_1\":60, \"nueronas_layer_2\":30, \"n_epochs\":100},\n",
    "    #\"prueba_3\": {\"n_pca\":100, \"excl_n_prim_comp\":0, \"nueronas_layer_1\":45, \"nueronas_layer_2\":26, \"n_epochs\":100},\n",
    "    #\"prueba_4\": {\"n_pca\":100, \"excl_n_prim_comp\":3, \"nueronas_layer_1\":45, \"nueronas_layer_2\":26, \"n_epochs\":100},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrego esto para realizar algunas pruebas en otro directorio pero utilizando las funciones definidas por fede...\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Obtener el path de ejecucion de la notebook\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define el nombre del proyecto\n",
    "root_dir_name = \"NuestrasCaras\"\n",
    "\n",
    "# Obtiene el path del proyecto\n",
    "while not os.path.basename(current_dir) == root_dir_name:\n",
    "    current_dir = os.path.dirname(current_dir)\n",
    "    \n",
    "# Agrega path a librerias\n",
    "sys.path.append(current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer imágenes para entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_imagenes = DIM\n",
    "data_dir = \"01_entrenar_modelo/fotos_entrenamiento\"\n",
    "nombres = []\n",
    "imagenes = []\n",
    "\n",
    "# Cargar imágenes y etiquetas\n",
    "dir = os.path.join(current_dir,\"01_entrenar_modelo\",\"fotos_entrenamiento\")\n",
    "for archivo in os.listdir(dir):\n",
    "    if archivo.endswith('.jpeg') or archivo.endswith('.jpg'):\n",
    "        nombre = archivo.split('_')[0].replace(\".jpg\",\"\").replace(\".jpeg\",\"\")\n",
    "        nombre = re.sub(r\"\\d+\", \"\", nombre)\n",
    "        ruta_imagen = os.path.join(dir, archivo)\n",
    "        imagen = Image.open(ruta_imagen)\n",
    "        imagen = np.array(imagen.resize((dim_imagenes, dim_imagenes)))  # Redimensionar imágenes para un tamaño uniforme\n",
    "        if len(imagen.shape) == 3 and imagen.shape[2] == 3:\n",
    "            imagen = cv2.cvtColor(imagen, cv2.COLOR_RGB2GRAY)  # Convertir a escala de grises si es necesario\n",
    "        imagenes.append(imagen.flatten())\n",
    "        nombres.append(nombre)\n",
    "\n",
    "imagenes_all = np.array(imagenes)\n",
    "nombres_all = np.array(nombres)\n",
    "\n",
    "# estand\n",
    "imagenes_all = imagenes_all/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10459, 900)\n",
      "(10459,)\n"
     ]
    }
   ],
   "source": [
    "print(imagenes_all.shape)\n",
    "print(nombres_all.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer imagenes para testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = (\"01_entrenar_modelo/fotos_test_entrenamiento\")\n",
    "nombres = []\n",
    "imagenes = []\n",
    "\n",
    "# Cargar imágenes y etiquetas\n",
    "dir = os.path.join(current_dir,data_dir)\n",
    "for archivo in os.listdir(dir):\n",
    "    if archivo.endswith('.jpeg') or archivo.endswith('.jpg'):\n",
    "        nombre = archivo.split('_')[0].replace(\".jpg\",\"\").replace(\".jpeg\",\"\")\n",
    "        nombre = re.sub(r\"\\d+\", \"\", nombre)\n",
    "        ruta_imagen = os.path.join(dir, archivo)\n",
    "        imagen = Image.open(ruta_imagen)\n",
    "        imagen = np.array(imagen.resize((dim_imagenes, dim_imagenes)))  # Redimensionar imágenes para un tamaño uniforme\n",
    "        if len(imagen.shape) == 3 and imagen.shape[2] == 3:\n",
    "            imagen = cv2.cvtColor(imagen, cv2.COLOR_RGB2GRAY)  # Convertir a escala de grises si es necesario\n",
    "        imagenes.append(imagen.flatten())\n",
    "        nombres.append(nombre)\n",
    "\n",
    "new_images = np.array(imagenes)\n",
    "nombres_new = np.array(nombres)\n",
    "\n",
    "# estand\n",
    "new_images = new_images/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 900)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "print(new_images.shape)\n",
    "print(nombres_new.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y testeo del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define funcion para entrenar \n",
    "def run_keras_model(n_pca, excl_n_prim_comp, nueronas_layer_1, nueronas_layer_2, n_epochs, imagenes, nombres, nuevas_imagenes, nuevos_nombres):\n",
    "    \n",
    "    # Dividir en entrenamiento y prueba\n",
    "    # se comentan ya que no se divide la base en train y test...\n",
    "    # se va a utilizar TODAS las imagenes para el entrenamiento...\n",
    "    # tenemos imagenes extras de todos los integrantes para realizar la evaluacion...\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(imagenes, nombres, test_size=0.05, random_state=42, stratify=nombres)\n",
    "    X_train = imagenes #np.array(imagenes)\n",
    "    y_train = nombres #np.array(nombres).reshape(-1)\n",
    "    \n",
    "    # Aplicar PCA\n",
    "    pca = PCA(n_components=n_pca, random_state=seed)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    #X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    # Escalar los datos \n",
    "    scaler = StandardScaler()\n",
    "    #scaler = MinMaxScaler() # en caso de ser necesario probar con otro metodo de escalado\n",
    "    X_train_pca_scaled = scaler.fit_transform(X_train_pca)\n",
    "    #X_test_pca_scaled = scaler.transform(X_test_pca)\n",
    "    \n",
    "    \n",
    "    # Codificar las etiquetas\n",
    "    encoder = LabelEncoder()\n",
    "    y_train_encoded = encoder.fit_transform(y_train)\n",
    "    #y_test_encoded = encoder.transform(y_test)\n",
    "    y_train_categorical = to_categorical(y_train_encoded, num_classes=19)\n",
    "    #y_test_categorical = to_categorical(y_test_encoded, num_classes=19)\n",
    "    \n",
    "    \n",
    "    # Definir la red neuronal\n",
    "    entreno_con = X_train_pca_scaled[:,excl_n_prim_comp:]\n",
    "    #testeo_con = X_test_pca_scaled[:,excl_n_prim_comp:]\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=entreno_con.shape[1:]))  # Definir la entrada del modelo\n",
    "    model.add(Dense(nueronas_layer_1, activation='sigmoid'))\n",
    "    model.add(Dense(nueronas_layer_2, activation='sigmoid'))\n",
    "    model.add(Dense(y_train_categorical.shape[1], activation='softmax'))\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    model.fit(entreno_con, y_train_categorical, epochs=n_epochs, batch_size=10, validation_split=0.3)\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    #loss, accuracy = model.evaluate(testeo_con, y_test_categorical)\n",
    "    #print(f'Precisión en el conjunto de prueba: {accuracy * 100:.2f}%')\n",
    "    \n",
    "    \n",
    "    # PREDICCIONES\n",
    "    # 2. Aplicar PCA\n",
    "    new_images_pca = pca.transform(nuevas_imagenes)\n",
    "    new_images_pca_scaled = scaler.transform(new_images_pca)\n",
    "    evaluo_con = new_images_pca_scaled[:,excl_n_prim_comp:]\n",
    "\n",
    "    # 3. Hacer predicciones\n",
    "    predictions = model.predict(evaluo_con)\n",
    "\n",
    "    # Obtener los nombres correspondientes a las clases predichas\n",
    "    predicted_names = encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "    # Imprimir las predicciones\n",
    "    total_predict = len(nuevos_nombres)\n",
    "    total_correcto = 0\n",
    "    for real, pred in zip(nuevos_nombres, predicted_names):\n",
    "        if real == pred:\n",
    "            total_correcto += 1\n",
    "    \n",
    "    # devuelve resultado corrida\n",
    "    resultado = total_correcto/total_predict\n",
    "    return resultado, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prueba_1: \n",
      "Epoch 1/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.2072 - loss: 2.6023 - val_accuracy: 0.0000e+00 - val_loss: 6.2861\n",
      "Epoch 2/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6124 - loss: 1.8246 - val_accuracy: 0.0000e+00 - val_loss: 7.3699\n",
      "Epoch 3/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7347 - loss: 1.2308 - val_accuracy: 0.0000e+00 - val_loss: 8.0892\n",
      "Epoch 4/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7868 - loss: 0.9042 - val_accuracy: 0.0000e+00 - val_loss: 8.6044\n",
      "Epoch 5/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8242 - loss: 0.7113 - val_accuracy: 0.0000e+00 - val_loss: 9.0223\n",
      "Epoch 6/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8584 - loss: 0.5810 - val_accuracy: 0.0000e+00 - val_loss: 9.3915\n",
      "Epoch 7/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8823 - loss: 0.4865 - val_accuracy: 0.0000e+00 - val_loss: 9.7349\n",
      "Epoch 8/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8989 - loss: 0.4143 - val_accuracy: 0.0000e+00 - val_loss: 10.0654\n",
      "Epoch 9/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9107 - loss: 0.3569 - val_accuracy: 0.0013 - val_loss: 10.3920\n",
      "Epoch 10/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9269 - loss: 0.3099 - val_accuracy: 0.0041 - val_loss: 10.7179\n",
      "Epoch 11/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9374 - loss: 0.2709 - val_accuracy: 0.0061 - val_loss: 11.0441\n",
      "Epoch 12/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9472 - loss: 0.2379 - val_accuracy: 0.0112 - val_loss: 11.3712\n",
      "Epoch 13/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9561 - loss: 0.2099 - val_accuracy: 0.0137 - val_loss: 11.6992\n",
      "Epoch 14/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9643 - loss: 0.1859 - val_accuracy: 0.0156 - val_loss: 12.0276\n",
      "Epoch 15/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9703 - loss: 0.1652 - val_accuracy: 0.0191 - val_loss: 12.3563\n",
      "Epoch 16/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9746 - loss: 0.1473 - val_accuracy: 0.0217 - val_loss: 12.6852\n",
      "Epoch 17/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9779 - loss: 0.1317 - val_accuracy: 0.0229 - val_loss: 13.0144\n",
      "Epoch 18/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9824 - loss: 0.1179 - val_accuracy: 0.0236 - val_loss: 13.3439\n",
      "Epoch 19/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9841 - loss: 0.1058 - val_accuracy: 0.0255 - val_loss: 13.6737\n",
      "Epoch 20/20\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9856 - loss: 0.0950 - val_accuracy: 0.0261 - val_loss: 14.0036\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prueba_1': 0.43333333333333335}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resutados = {}\n",
    "modelos = {}\n",
    "for prueba, params in pruebas.items():\n",
    "    print(f\"{prueba}: \")\n",
    "    resultado, modelo = run_keras_model(n_pca=params.get('n_pca'), \n",
    "                    excl_n_prim_comp=params.get('excl_n_prim_comp'), \n",
    "                    nueronas_layer_1=params.get('nueronas_layer_1'),\n",
    "                    nueronas_layer_2=params.get('nueronas_layer_2'),\n",
    "                    n_epochs=params.get('n_epochs'),\n",
    "                    imagenes=imagenes_all,\n",
    "                    nombres=nombres_all,\n",
    "                    nuevas_imagenes=new_images,\n",
    "                    nuevos_nombres=nombres_new\n",
    "                    )\n",
    "    resutados[prueba]=resultado\n",
    "    modelos[prueba]=modelo\n",
    "resutados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segundo testeo del modelo con nuevas imagenes del equipo \"Grupo 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 900)\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = (\"02_probar_nuevas_fotos/fotos_prueba_recortadas\")\n",
    "nombres = []\n",
    "imagenes = []\n",
    "\n",
    "# Cargar imágenes y etiquetas\n",
    "dir = os.path.join(current_dir,data_dir)\n",
    "for archivo in os.listdir(dir):\n",
    "    if archivo.endswith('.jpeg') or archivo.endswith('.jpg'):\n",
    "        nombre = archivo.split('_')[0].replace(\".jpg\",\"\").replace(\".jpeg\",\"\")\n",
    "        nombre = re.sub(r\"\\d+\", \"\", nombre)\n",
    "        ruta_imagen = os.path.join(dir, archivo)\n",
    "        imagen = Image.open(ruta_imagen)\n",
    "        imagen = np.array(imagen.resize((dim_imagenes, dim_imagenes)))  # Redimensionar imágenes para un tamaño uniforme\n",
    "        if len(imagen.shape) == 3 and imagen.shape[2] == 3:\n",
    "            imagen = cv2.cvtColor(imagen, cv2.COLOR_RGB2GRAY)  # Convertir a escala de grises si es necesario\n",
    "        imagenes.append(imagen.flatten())\n",
    "        nombres.append(nombre)\n",
    "\n",
    "new_images = np.array(imagenes)\n",
    "nombres_new = np.array(nombres)\n",
    "\n",
    "# estand\n",
    "new_images = new_images/255.0\n",
    "print(new_images.shape)\n",
    "print(nombres_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nombres_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define funcion para las predicciones\n",
    "def get_preds_predict2(model, n_pca, excl_n_prim_comp, imagenes, nombres, nuevas_imagenes, nuevos_nombres):\n",
    "    \n",
    "    # Dividir en entrenamiento y prueba\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(imagenes, nombres, test_size=0.05, random_state=42, stratify=nombres)\n",
    "    X_train = imagenes#np.array(imagenes)\n",
    "    y_train = nombres#np.array(nombres)\n",
    "\n",
    "    # Aplicar PCA\n",
    "    pca = PCA(n_components=n_pca, random_state=12)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    #X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    # Escalar los datos \n",
    "    scaler = StandardScaler()\n",
    "    #scaler = MinMaxScaler()\n",
    "    X_train_pca_scaled = scaler.fit_transform(X_train_pca)\n",
    "    #X_test_pca_scaled = scaler.transform(X_test_pca)\n",
    "    \n",
    "    \n",
    "    # Codificar las etiquetas\n",
    "    encoder = LabelEncoder()\n",
    "    y_train_encoded = encoder.fit_transform(y_train)\n",
    "    #y_test_encoded = encoder.transform(y_test)\n",
    "    #y_train_categorical = to_categorical(y_train_encoded, num_classes=19)\n",
    "    #y_test_categorical = to_categorical(y_test_encoded, num_classes=18)\n",
    "    \n",
    "    \n",
    "    # Definir la red neuronal\n",
    "    #entreno_con = X_train_pca_scaled[:,excl_n_prim_comp:]\n",
    "    #testeo_con = X_test_pca_scaled[:,excl_n_prim_comp:]    \n",
    "    \n",
    "    # PREDICCIONES\n",
    "    # 2. Aplicar PCA\n",
    "    new_images_pca = pca.transform(nuevas_imagenes)\n",
    "    new_images_pca_scaled = scaler.transform(new_images_pca)\n",
    "    evaluo_con = new_images_pca_scaled[:,excl_n_prim_comp:]\n",
    "\n",
    "    # 3. Hacer predicciones\n",
    "    predictions = model.predict(evaluo_con)\n",
    "\n",
    "    # Obtener los nombres correspondientes a las clases predichas\n",
    "    predicted_names = encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "    # Imprimir las predicciones\n",
    "    total_predict = len(nuevos_nombres)\n",
    "    total_correcto = 0\n",
    "    for real, pred in zip(nuevos_nombres, predicted_names):\n",
    "        if real == pred:\n",
    "            total_correcto += 1\n",
    "         \n",
    "    # df predicciones        \n",
    "    df_preds = pd.DataFrame(predictions.round(2))\n",
    "    class_names = encoder.classes_\n",
    "    df_preds.columns = class_names\n",
    "    \n",
    "    # devuelve resultado corrida\n",
    "    resultado = total_correcto/total_predict\n",
    "    return resultado, df_preds, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
     ]
    }
   ],
   "source": [
    "# SOLO SE MODIFICA MANUALMENTE EL MODELO SELECCIONADO!\n",
    "prueba_seleccionada = \"prueba_1\"\n",
    "\n",
    "modelo_seleccionado = modelos[prueba_seleccionada]\n",
    "\n",
    "# Obtener resultados\n",
    "n_pca_modelo_seleccionado = pruebas[prueba_seleccionada].get(\"n_pca\")\n",
    "excl_n_prim_comp_modelo_seleccionado = pruebas[prueba_seleccionada].get(\"excl_n_prim_comp\")\n",
    "resultado, df_preds, predicted_names = get_preds_predict2(\n",
    "    model=modelo_seleccionado, \n",
    "    n_pca=n_pca_modelo_seleccionado, \n",
    "    excl_n_prim_comp=excl_n_prim_comp_modelo_seleccionado, \n",
    "    imagenes=imagenes_all, \n",
    "    nombres=nombres_all, \n",
    "    nuevas_imagenes=new_images, \n",
    "    nuevos_nombres=nombres_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proporción de aciertos:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proporción de aciertos\n",
    "print(\"Proporción de aciertos:\")\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones (cada fila es una foto):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abel</th>\n",
       "      <th>carlos</th>\n",
       "      <th>federicoG</th>\n",
       "      <th>federicoR</th>\n",
       "      <th>florencia</th>\n",
       "      <th>francoA</th>\n",
       "      <th>francoS</th>\n",
       "      <th>gerard</th>\n",
       "      <th>gustavo</th>\n",
       "      <th>joaquin</th>\n",
       "      <th>juan</th>\n",
       "      <th>lautaro</th>\n",
       "      <th>lisandro</th>\n",
       "      <th>marco</th>\n",
       "      <th>matias</th>\n",
       "      <th>natalia</th>\n",
       "      <th>noelia</th>\n",
       "      <th>paola</th>\n",
       "      <th>victorio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lautaro</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lautaro</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lautaro</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lautaro</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paola</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paola</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         abel  carlos  federicoG  federicoR  florencia  francoA  francoS  \\\n",
       "lautaro   0.0    0.02       0.00       0.00       0.00      0.0     0.00   \n",
       "lautaro   0.0    0.00       0.00       0.00       0.00      0.0     0.00   \n",
       "lautaro   0.0    0.01       0.00       0.00       0.01      0.0     0.00   \n",
       "lautaro   0.0    0.00       0.00       0.00       0.01      0.0     0.00   \n",
       "paola     0.0    0.00       0.00       0.00       0.53      0.0     0.00   \n",
       "paola     0.1    0.00       0.03       0.02       0.61      0.2     0.01   \n",
       "\n",
       "         gerard  gustavo  joaquin  juan  lautaro  lisandro  marco  matias  \\\n",
       "lautaro    0.00     0.00     0.00   0.0     0.97      0.00   0.00     0.0   \n",
       "lautaro    0.00     0.00     0.00   0.0     1.00      0.00   0.00     0.0   \n",
       "lautaro    0.01     0.00     0.00   0.0     0.98      0.00   0.00     0.0   \n",
       "lautaro    0.12     0.00     0.00   0.0     0.19      0.66   0.01     0.0   \n",
       "paola      0.07     0.00     0.02   0.0     0.37      0.01   0.00     0.0   \n",
       "paola      0.00     0.01     0.00   0.0     0.00      0.02   0.00     0.0   \n",
       "\n",
       "         natalia  noelia  paola  victorio  \n",
       "lautaro      0.0     0.0    0.0       0.0  \n",
       "lautaro      0.0     0.0    0.0       0.0  \n",
       "lautaro      0.0     0.0    0.0       0.0  \n",
       "lautaro      0.0     0.0    0.0       0.0  \n",
       "paola        0.0     0.0    0.0       0.0  \n",
       "paola        0.0     0.0    0.0       0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicciones\n",
    "print(\"Predicciones (cada fila es una foto):\")\n",
    "df_preds.index = nombres_new\n",
    "df_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuestras_caras_grupo_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
